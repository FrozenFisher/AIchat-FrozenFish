"""
AIèŠå¤©æœåŠ¡å™¨ - è´Ÿè´£æ‰€æœ‰è®¡ç®—ä»»åŠ¡
æä¾›LLMæ¨ç†ã€éŸ³é¢‘ç”Ÿæˆã€æ¨¡å‹ç®¡ç†ç­‰åŠŸèƒ½
æ”¯æŒæœ¬åœ°Ollamaå’Œåœ¨çº¿DeepSeek APIä¸¤ç§æ¨¡å¼
"""

import os
import re
import yaml
import json
import uuid
import asyncio
import threading
import base64
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from pathlib import Path
from queue import Queue
import time

import requests
import tiktoken
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import openai
import simpleaudio as sa

# BERTæƒ…æ„Ÿåˆ†æç›¸å…³å¯¼å…¥
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline
import torch

# æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    message: str
    agent: str = "é“¶ç‹¼"
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    audio_data: Optional[List[str]] = None  # base64ç¼–ç çš„éŸ³é¢‘æ•°æ®

class AgentConfig(BaseModel):
    name: str
    gpt_path: str
    sovits_path: str
    bg_path: str
    prompt_path: str
    ref_audio_path: Dict[str, str]  # ä¿®æ”¹ä¸ºå­—å…¸æ ¼å¼ï¼Œæ”¯æŒå¤šç§æƒ…æ„Ÿ

class ModelConfig(BaseModel):
    gpt_weights: str
    sovits_weights: str

# å…¨å±€å˜é‡
app = FastAPI(title="AIèŠå¤©æœåŠ¡å™¨", version="1.0.0")
ollama_client = None
deepseek_client = None
model_name = "qwen3:8b"  # é»˜è®¤ä½¿ç”¨qwen3:8bæ¨¡å‹
online_mode = False  # åœ¨çº¿æ¨¡å¼æ ‡å¿—
deepseek_api_key = None  # DeepSeek APIå¯†é’¥
chat_sessions: Dict[str, List[ChatMessage]] = {}
current_agent = "é“¶ç‹¼"
agent_configs: Dict[str, AgentConfig] = {}

# éŸ³é¢‘ç”Ÿæˆé˜Ÿåˆ—å’Œé”ï¼ˆè§£å†³GPT-SoVITSå¹¶å‘é—®é¢˜ï¼‰
audio_generation_lock = threading.Lock()
audio_generation_queue = Queue()

# BERTæƒ…æ„Ÿåˆ†æç›¸å…³å…¨å±€å˜é‡
bert_tokenizer = None
bert_model = None
sentiment_analyzer = None
emotion_map = {
    0: "é«˜å…´", 1: "æ‚²ä¼¤", 2: "æ„¤æ€’", 3: "æƒŠè®¶", 4: "ææƒ§", 
    5: "åŒæ¶", 6: "ä¸­æ€§", 7: "å®³ç¾", 8: "å…´å¥‹", 9: "èˆ’é€‚",
    10: "ç´§å¼ ", 11: "çˆ±æ…•", 12: "å§”å±ˆ", 13: "éª„å‚²", 14: "å›°æƒ‘"
}

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def load_bert_model():
    """åŠ è½½BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    global bert_tokenizer, bert_model, sentiment_analyzer
    
    try:
        model_path = "./lib/retrainedBERT"
        if not os.path.exists(model_path):
            print(f"âŒ BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        print(f"æ­£åœ¨åŠ è½½BERTæ¨¡å‹: {model_path}")
        bert_tokenizer = BertTokenizer.from_pretrained(model_path)
        bert_model = BertForSequenceClassification.from_pretrained(model_path)
        
        # åˆ›å»ºæƒ…æ„Ÿåˆ†æpipeline
        sentiment_analyzer = pipeline(
            "text-classification",
            model=bert_model,
            tokenizer=bert_tokenizer,
            framework="pt",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        
        print(f"âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        return True
        
    except Exception as e:
        print(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def predict_emotion(text: str) -> str:
    """é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿï¼Œè¿”å›æƒ…æ„Ÿæ ‡ç­¾"""
    global sentiment_analyzer, emotion_map
    
    if sentiment_analyzer is None:
        print("BERTæ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤ä¸­æ€§æƒ…æ„Ÿ")
        return "ä¸­æ€§"
    
    try:
        if not text.strip():
            return "ä¸­æ€§"
        
        result = sentiment_analyzer(text)
        label = int(result[0]['label'].split('_')[-1])
        emotion = emotion_map.get(label, "ä¸­æ€§")
        
        print(f"æƒ…æ„Ÿåˆ†æ: '{text[:50]}...' -> {emotion} (ç½®ä¿¡åº¦: {result[0]['score']:.4f})")
        return emotion
        
    except Exception as e:
        print(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        return "ä¸­æ€§"

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    global agent_configs
    current_path = Path(__file__).parent
    config_path = current_path / "modelconfig.yaml"
    
    with open(config_path, 'r', encoding='utf-8') as file:
        config = yaml.safe_load(file)
    
    for agent_name, agent_data in config['Agents'].items():
        agent_configs[agent_name] = AgentConfig(
            name=agent_name,
            gpt_path=str(current_path.parent / "GPT-SoVITS" / agent_data["GPTPath"]),
            sovits_path=str(current_path.parent / "GPT-SoVITS" / agent_data["SoVITSPath"]),
            bg_path=str(current_path / agent_data["bgPath"]),
            prompt_path=str(current_path / agent_data["promptPath"]),
            ref_audio_path=agent_data["refaudioPath"]  # ç°åœ¨æ˜¯å­—å…¸æ ¼å¼
        )

def init_ollama():
    """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
    global ollama_client
    
    # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡è¿æ¥æˆåŠŸ")
            ollama_client = True
        else:
            print("âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸")
            ollama_client = False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
        ollama_client = False

def init_deepseek():
    """åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯"""
    global deepseek_client, deepseek_api_key
    
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    if not deepseek_api_key:
        print("âŒ æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        deepseek_client = None
        return
    
    try:
        # åˆ›å»ºDeepSeekå®¢æˆ·ç«¯
        deepseek_client = openai.OpenAI(
            api_key=deepseek_api_key,
            base_url="https://api.deepseek.com/v1"
        )
        
        # æµ‹è¯•è¿æ¥
        response = deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10
        )
        print("âœ… DeepSeek APIè¿æ¥æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ DeepSeek APIè¿æ¥å¤±è´¥: {e}")
        deepseek_client = None

def get_ollama_client():
    """è·å–Ollamaå®¢æˆ·ç«¯é…ç½®"""
    return {
        "base_url": "http://localhost:11434",
        "model": model_name
    }

def split_text_for_audio(text: str) -> tuple[List[str], List[str]]:
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆéŸ³é¢‘ç”Ÿæˆçš„å¥å­
    è¿”å›: (bert_sentences, audio_sentences)
    bert_sentences: åŒ…å«åœ†æ‹¬å·å†…å®¹çš„å¥å­ï¼Œç”¨äºBERTæƒ…æ„Ÿé¢„æµ‹
    audio_sentences: å»é™¤åœ†æ‹¬å·å†…å®¹çš„å¥å­ï¼Œç”¨äºéŸ³é¢‘ç”Ÿæˆ
    """
    # è¿‡æ»¤æ‰<think></think>æ ‡ç­¾åŠå…¶å†…å®¹
    text_without_think = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # å…ˆæŒ‰æ ‡ç‚¹åˆ†å¥ï¼Œä¿ç•™æ‹¬å·å†…å®¹ï¼ˆæ·»åŠ ~ä½œä¸ºå¥å­ç»“æŸç¬¦ï¼‰
    bert_sentences = re.findall(r'[^.!?;:ã€‚ï¼ï¼Ÿï¼šï¼›~]*[.!?;:ã€‚ï¼ï¼Ÿï¼šï¼›~]+', text_without_think)
    bert_sentences = [s.strip() for s in bert_sentences if s.strip()]
    # å¯¹æ¯ä¸ªå¥å­åˆ†åˆ«å»é™¤æ‹¬å·å†…å®¹
    audio_sentences = [re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', s).strip() for s in bert_sentences]
    return bert_sentences, audio_sentences

async def generate_audio_segments(text: str, agent: str) -> List[str]:
    """ç”ŸæˆéŸ³é¢‘ç‰‡æ®µï¼Œè¿”å›base64ç¼–ç çš„éŸ³é¢‘æ•°æ®åˆ—è¡¨"""
    agent_config = agent_configs.get(agent)
    if not agent_config or not agent_config.ref_audio_path:
        return []
    
    bert_sentences, audio_sentences = split_text_for_audio(text)
    audio_data_list = []
    
    # ç¡®ä¿ä¸¤ä¸ªåˆ—è¡¨é•¿åº¦ä¸€è‡´
    if len(bert_sentences) != len(audio_sentences):
        print(f"è­¦å‘Šï¼šBERTå¥å­æ•°é‡({len(bert_sentences)})ä¸éŸ³é¢‘å¥å­æ•°é‡({len(audio_sentences)})ä¸åŒ¹é…")
        return []
    
    for i, (bert_sentence, audio_sentence) in enumerate(zip(bert_sentences, audio_sentences)):
        try:
            # ä½¿ç”¨BERTæ¨¡å‹é¢„æµ‹æƒ…æ„Ÿï¼ˆä½¿ç”¨åŒ…å«åœ†æ‹¬å·å†…å®¹çš„å¥å­ï¼‰
            emotion = predict_emotion(bert_sentence)
            print(f"å¥å­ {i+1}: BERTè¾“å…¥ '{bert_sentence}' -> æƒ…æ„Ÿ: {emotion}")
            print(f"å¥å­ {i+1}: éŸ³é¢‘è¾“å…¥ '{audio_sentence}'")
            
            # æ ¹æ®æƒ…æ„Ÿé€‰æ‹©å¯¹åº”çš„å‚è€ƒéŸ³é¢‘
            ref_audio_path = agent_config.ref_audio_path.get(emotion)
            if not ref_audio_path:
                print(f"æœªæ‰¾åˆ°æƒ…æ„Ÿ '{emotion}' çš„å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨ä¸­æ€§éŸ³é¢‘")
                ref_audio_path = agent_config.ref_audio_path.get("ä¸­æ€§")
            
            if not ref_audio_path or not os.path.exists(ref_audio_path):
                print(f"å‚è€ƒéŸ³é¢‘ä¸å­˜åœ¨: {ref_audio_path}")
                continue
            
            # è·å–å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬ï¼ˆä»æ–‡ä»¶åæå–ï¼‰
            ref_audio_name = Path(ref_audio_path).stem
            current_path = Path(__file__).parent
            
            # ä½¿ç”¨é”ç¡®ä¿GPT-SoVITS APIè°ƒç”¨æ˜¯ä¸²è¡Œçš„
            try:
                with audio_generation_lock:
                    print(f"ğŸ”’ è·å–éŸ³é¢‘ç”Ÿæˆé”ï¼Œå¼€å§‹ç”Ÿæˆç¬¬ {i+1} ä¸ªéŸ³é¢‘...")
                    
                    # è°ƒç”¨GPT-SoVITS APIï¼ˆä½¿ç”¨å»é™¤åœ†æ‹¬å·å†…å®¹çš„å¥å­ï¼‰
                    tts_url = "http://127.0.0.1:9880/tts"
                    post_data = {
                        "prompt_text": ref_audio_name,
                        "prompt_lang": "zh",
                        "ref_audio_path": str(current_path / ref_audio_path),
                        "text": audio_sentence,
                        "text_lang": "zh",
                        "parallel_infer": True,  # å¯ç”¨å¹¶è¡Œæ¨ç†
                    }
                    
                    response = requests.post(tts_url, json=post_data)
                    if response.status_code == 200:
                        # å°†éŸ³é¢‘æ•°æ®ç¼–ç ä¸ºbase64å­—ç¬¦ä¸²
                        audio_base64 = base64.b64encode(response.content).decode('utf-8')
                        audio_data_list.append(audio_base64)
                        print(f"âœ… ç”ŸæˆéŸ³é¢‘: {audio_sentence} (æƒ…æ„Ÿ: {emotion})")
                    else:
                        print(f"âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {response.status_code}")
                        print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
                    
                    print(f"ğŸ”“ é‡Šæ”¾éŸ³é¢‘ç”Ÿæˆé”ï¼Œç¬¬ {i+1} ä¸ªéŸ³é¢‘ç”Ÿæˆå®Œæˆ")
            except Exception as lock_error:
                print(f"âŒ éŸ³é¢‘ç”Ÿæˆé”é”™è¯¯: {lock_error}")
                # ç¡®ä¿é”è¢«é‡Šæ”¾
                if audio_generation_lock.locked():
                    audio_generation_lock.release()
                    print("ğŸ”“ å¼ºåˆ¶é‡Šæ”¾éŸ³é¢‘ç”Ÿæˆé”")
                
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç”Ÿæˆé”™è¯¯: {e}")
    
    return audio_data_list

async def call_llm_api(messages: List[Dict]) -> str:
    """è°ƒç”¨LLM APIï¼ˆæ”¯æŒOllamaå’ŒDeepSeekï¼‰"""
    global online_mode
    
    if online_mode and deepseek_client is not None:
        # åœ¨çº¿æ¨¡å¼ï¼šä½¿ç”¨DeepSeek API
        try:
            # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
            formatted_messages = []
            for msg in messages:
                formatted_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",  # ä½¿ç”¨DeepSeek-V3æ¨¡å‹
                messages=formatted_messages,
                temperature=0.8,
                max_tokens=1024
            )
            content = response.choices[0].message.content
            if content is None:
                raise Exception("DeepSeek APIè¿”å›ç©ºå†…å®¹")
            return content
            
        except Exception as e:
            print(f"DeepSeek APIé”™è¯¯: {e}")
            raise
    else:
        # ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨Ollama API
        if not ollama_client:
            raise Exception("OllamaæœåŠ¡ä¸å¯ç”¨")
            
        try:
            # æ„å»ºOllama APIè¯·æ±‚
            ollama_data = {
                "model": model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.8,
                    "num_predict": 1024
                }
            }
            
            response = requests.post(
                "http://localhost:11434/api/chat",
                json=ollama_data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["message"]["content"]
            else:
                raise Exception(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"Ollama APIé”™è¯¯: {e}")
            raise

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    global online_mode
    
    load_config()
    init_ollama()
    init_deepseek()
    
    # åŠ è½½BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹
    bert_loaded = load_bert_model()
    if bert_loaded:
        print("âœ… BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸ BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤ä¸­æ€§æƒ…æ„Ÿ")
    
    # æ ¹æ®ç¯å¢ƒå˜é‡è®¾ç½®åœ¨çº¿æ¨¡å¼
    online_mode = os.getenv("ONLINE_MODE", "false").lower() == "true"
    
    if online_mode:
        if deepseek_client is not None:
            print("âœ… æœåŠ¡å™¨å¯åŠ¨å®Œæˆ - åœ¨çº¿æ¨¡å¼ (DeepSeek API)")
        else:
            print("âŒ åœ¨çº¿æ¨¡å¼é…ç½®å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼")
            online_mode = False
    else:
        if ollama_client:
            print("âœ… æœåŠ¡å™¨å¯åŠ¨å®Œæˆ - ç¦»çº¿æ¨¡å¼ (Ollama)")
        else:
            print("âŒ ç¦»çº¿æ¨¡å¼é…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    print("æœåŠ¡å™¨å…³é—­")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    mode = "åœ¨çº¿æ¨¡å¼ (DeepSeek API)" if online_mode else "ç¦»çº¿æ¨¡å¼ (Ollama)"
    return {
        "message": "AIèŠå¤©æœåŠ¡å™¨è¿è¡Œä¸­", 
        "version": "1.0.0", 
        "mode": mode,
        "framework": "DeepSeek API" if online_mode else "Ollama"
    }

@app.get("/agents")
async def get_agents():
    """è·å–æ‰€æœ‰å¯ç”¨è§’è‰²"""
    return {"agents": list(agent_configs.keys())}

@app.get("/agent/{agent_name}")
async def get_agent_config(agent_name: str):
    """è·å–æŒ‡å®šè§’è‰²çš„é…ç½®"""
    if agent_name not in agent_configs:
        raise HTTPException(status_code=404, detail="è§’è‰²ä¸å­˜åœ¨")
    return agent_configs[agent_name]

@app.get("/models")
async def get_models():
    """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json()
            return {"models": [model["name"] for model in models["models"]]}
        else:
            return {"models": []}
    except Exception as e:
        print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return {"models": []}

@app.post("/models/{new_model_name}")
async def switch_model(new_model_name: str):
    """åˆ‡æ¢Ollamaæ¨¡å‹"""
    global model_name
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json()
            available_models = [model["name"] for model in models["models"]]
            if new_model_name in available_models:
                model_name = new_model_name
                return {"message": f"æˆåŠŸåˆ‡æ¢åˆ°æ¨¡å‹: {model_name}"}
            else:
                raise HTTPException(status_code=404, detail="æ¨¡å‹ä¸å­˜åœ¨")
        else:
            raise HTTPException(status_code=500, detail="æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    global current_agent
    
    # æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
    if not ollama_client:
        raise HTTPException(status_code=503, detail="OllamaæœåŠ¡ä¸å¯ç”¨")
    
    # æ›´æ–°å½“å‰è§’è‰²
    if request.agent != current_agent:
        current_agent = request.agent
        await switch_agent(request.agent)
    
    # è·å–æˆ–åˆ›å»ºä¼šè¯
    session_id = request.session_id or str(uuid.uuid4())
    if session_id not in chat_sessions:
        chat_sessions[session_id] = []
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    chat_sessions[session_id].append(
        ChatMessage(role="user", content=request.message)
    )
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [{"role": msg.role, "content": msg.content} 
               for msg in chat_sessions[session_id]]
    
    try:
        # è°ƒç”¨Ollama API
        response_content = await call_llm_api(messages)
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°ä¼šè¯å†å²
        chat_sessions[session_id].append(
            ChatMessage(role="assistant", content=response_content)
        )
        
        # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        audio_data = []
        if request.agent != "userinput":
            audio_data = await generate_audio_segments(response_content, request.agent)
        
        return ChatResponse(
            response=response_content,
            session_id=session_id,
            audio_data=audio_data
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

@app.post("/switch_agent/{agent_name}")
async def switch_agent(agent_name: str):
    """åˆ‡æ¢è§’è‰²"""
    global current_agent
    
    if agent_name not in agent_configs:
        raise HTTPException(status_code=404, detail="è§’è‰²ä¸å­˜åœ¨")
    
    agent_config = agent_configs[agent_name]
    
    try:
        # åˆ‡æ¢GPTæ¨¡å‹
        if agent_config.gpt_path != "none":
            gpt_url = "http://127.0.0.1:9880/set_gpt_weights"
            params = {"weights_path": Path(agent_config.gpt_path).name}
            response = requests.get(gpt_url, params=params)
            if response.status_code != 200:
                print(f"GPTæ¨¡å‹åˆ‡æ¢å¤±è´¥: {response.text}")
        
        # åˆ‡æ¢SoVITSæ¨¡å‹
        if agent_config.sovits_path != "none":
            sovits_url = "http://127.0.0.1:9880/set_sovits_weights"
            params = {"weights_path": Path(agent_config.sovits_path).name}
            response = requests.get(sovits_url, params=params)
            if response.status_code != 200:
                print(f"SoVITSæ¨¡å‹åˆ‡æ¢å¤±è´¥: {response.text}")
        
        current_agent = agent_name
        return {"message": f"æˆåŠŸåˆ‡æ¢åˆ°è§’è‰²: {agent_name}"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§’è‰²åˆ‡æ¢å¤±è´¥: {str(e)}")

@app.get("/audio/{filename}")
async def get_audio(filename: str):
    """è·å–éŸ³é¢‘æ–‡ä»¶ - å·²åºŸå¼ƒï¼Œç°åœ¨ç›´æ¥è¿”å›éŸ³é¢‘æ•°æ®"""
    raise HTTPException(status_code=410, detail="æ­¤APIå·²åºŸå¼ƒï¼ŒéŸ³é¢‘æ•°æ®ç°åœ¨ç›´æ¥åŒ…å«åœ¨èŠå¤©å“åº”ä¸­")

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤ä¼šè¯"""
    if session_id in chat_sessions:
        del chat_sessions[session_id]
        return {"message": "ä¼šè¯å·²åˆ é™¤"}
    else:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

@app.get("/sessions")
async def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
    return {"sessions": list(chat_sessions.keys())}

@app.delete("/audio/{filename}")
async def delete_audio(filename: str):
    """åˆ é™¤éŸ³é¢‘æ–‡ä»¶ - å·²åºŸå¼ƒï¼ŒéŸ³é¢‘æ•°æ®ä¸å†ä¿å­˜ä¸ºæ–‡ä»¶"""
    raise HTTPException(status_code=410, detail="æ­¤APIå·²åºŸå¼ƒï¼ŒéŸ³é¢‘æ•°æ®ä¸å†ä¿å­˜ä¸ºæ–‡ä»¶")

@app.get("/agent/{agent_name}/prompt")
async def get_agent_prompt(agent_name: str):
    """è·å–æŒ‡å®šè§’è‰²çš„æç¤ºè¯"""
    if agent_name not in agent_configs:
        raise HTTPException(status_code=404, detail="è§’è‰²ä¸å­˜åœ¨")
    
    agent_config = agent_configs[agent_name]
    if agent_config.prompt_path == "none":
        return {"prompt": "", "message": "è¯¥è§’è‰²æ²¡æœ‰é…ç½®æç¤ºè¯"}
    
    try:
        with open(agent_config.prompt_path, 'r', encoding='utf-8') as f:
            prompt = f.read()
        return {"prompt": prompt, "message": "æˆåŠŸè·å–è§’è‰²æç¤ºè¯"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯»å–æç¤ºè¯å¤±è´¥: {str(e)}")

@app.post("/agent/{agent_name}/init")
async def init_agent_session(agent_name: str, request: ChatRequest):
    """åˆå§‹åŒ–è§’è‰²ä¼šè¯ï¼Œå‘é€è§’è‰²æç¤ºè¯"""
    global current_agent
    
    # æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
    if not ollama_client:
        raise HTTPException(status_code=503, detail="OllamaæœåŠ¡ä¸å¯ç”¨")
    
    # æ›´æ–°å½“å‰è§’è‰²
    if agent_name != current_agent:
        current_agent = agent_name
        await switch_agent(agent_name)
    
    # è·å–è§’è‰²æç¤ºè¯
    if agent_name not in agent_configs:
        raise HTTPException(status_code=404, detail="è§’è‰²ä¸å­˜åœ¨")
    
    agent_config = agent_configs[agent_name]
    if agent_config.prompt_path == "none":
        return ChatResponse(
            response="è¯¥è§’è‰²æ²¡æœ‰é…ç½®æç¤ºè¯ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹å¯¹è¯ã€‚",
            session_id=request.session_id or str(uuid.uuid4()),
            audio_data=[]
        )
    
    try:
        with open(agent_config.prompt_path, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
        
        # åˆ›å»ºä¼šè¯å¹¶å‘é€ç³»ç»Ÿæç¤ºè¯
        session_id = request.session_id or str(uuid.uuid4())
        if session_id not in chat_sessions:
            chat_sessions[session_id] = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        chat_sessions[session_id].append(
            ChatMessage(role="system", content=system_prompt)
        )
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": msg.role, "content": msg.content} 
                   for msg in chat_sessions[session_id]]
        
        # è°ƒç”¨Ollama APIè·å–è§’è‰²åˆå§‹åŒ–å›å¤
        response_content = await call_llm_api(messages)
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°ä¼šè¯å†å²
        chat_sessions[session_id].append(
            ChatMessage(role="assistant", content=response_content)
        )
        
        # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        audio_data = []
        if agent_name != "userinput":
            audio_data = await generate_audio_segments(response_content, agent_name)
        
        return ChatResponse(
            response=response_content,
            session_id=session_id,
            audio_data=audio_data
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§’è‰²åˆå§‹åŒ–å¤±è´¥: {str(e)}")

@app.post("/config/online")
async def set_online_mode(online: bool):
    """è®¾ç½®åœ¨çº¿æ¨¡å¼"""
    global online_mode
    online_mode = online
    
    if online_mode:
        if deepseek_client is not None:
            return {"message": "å·²åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼ (DeepSeek API)", "mode": "online"}
        else:
            raise HTTPException(status_code=400, detail="DeepSeek APIæœªé…ç½®æˆ–è¿æ¥å¤±è´¥")
    else:
        if ollama_client:
            return {"message": "å·²åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼ (Ollama)", "mode": "offline"}
        else:
            raise HTTPException(status_code=400, detail="OllamaæœåŠ¡ä¸å¯ç”¨")

@app.get("/config/mode")
async def get_mode():
    """è·å–å½“å‰æ¨¡å¼"""
    mode = "online" if online_mode else "offline"
    framework = "DeepSeek API" if online_mode else "Ollama"
    return {"mode": mode, "framework": framework}

@app.post("/emotion/analyze")
async def analyze_emotion(text: str):
    """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
    try:
        emotion = predict_emotion(text)
        return {
            "text": text,
            "emotion": emotion,
            "message": "æƒ…æ„Ÿåˆ†æå®Œæˆ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")

@app.get("/emotion/status")
async def get_emotion_status():
    """è·å–æƒ…æ„Ÿåˆ†ææ¨¡å‹çŠ¶æ€"""
    return {
        "bert_loaded": sentiment_analyzer is not None,
        "device": "CUDA" if torch.cuda.is_available() else "CPU",
        "available_emotions": list(emotion_map.values())
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)